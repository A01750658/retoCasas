{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e6eb33",
   "metadata": {},
   "source": [
    "# ElasticNet + LGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b1284",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c124ab1f",
   "metadata": {},
   "source": [
    "Importamos librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e8a9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import r2_score, root_mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "from lightgbm import LGBMRegressor as LGBM\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "sys.path.append('../../../utils/')\n",
    "from utils_yose import build_preprocessor\n",
    "from utils_yose import MODEL_NAME\n",
    "from mlflow_setup import setup_mlflow\n",
    "\n",
    "sys.path.append('../../ensemble_2p/')\n",
    "from model_ensemble.ensemble import WeightedEnsemble\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9892f758",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baebaa2",
   "metadata": {},
   "source": [
    "Cargamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "757bd8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"../../../../data/housing_data/train.csv\"\n",
    "test_data = \"../../../../data/housing_data/test.csv\"\n",
    "\n",
    "if os.path.exists(train_data):\n",
    "    df_train = pd.read_csv(train_data)\n",
    "else:\n",
    "    print(\"No se encuentra el archivo de entrenamiento\")\n",
    "    \n",
    "if os.path.exists(test_data):\n",
    "    df_test = pd.read_csv(test_data)\n",
    "    X_test = df_test.drop([\"Id\"], axis=1)\n",
    "else:\n",
    "    print(\"No se encuentra el archivo de prueba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e44b7b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73f53ad",
   "metadata": {},
   "source": [
    "Modelos base con los que se va a trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb674431",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log1p(df_train[\"SalePrice\"]).astype(float)\n",
    "X = df_train.drop([\"SalePrice\", \"Id\"], axis=1)\n",
    "\n",
    "rstate = 42\n",
    "base_models = {\n",
    "    \"elasticnet\": ElasticNet(alpha=0.0005, l1_ratio=0.9, random_state=rstate),\n",
    "    \"lgbm\": LGBM(n_estimators=3000, learning_rate=0.03, max_depth=-1,\n",
    "                 num_leaves=31, subsample=0.8, colsample_bytree=0.8,\n",
    "                 random_state=rstate, n_jobs=-1)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea5e0d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b13dc",
   "metadata": {},
   "source": [
    "Entrenamiento del modelo con Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bba9a51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3373\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 183\n",
      "[LightGBM] [Info] Start training from score 12.025324\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3358\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 185\n",
      "[LightGBM] [Info] Start training from score 12.028659\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002850 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3364\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 183\n",
      "[LightGBM] [Info] Start training from score 12.021956\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003189 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3361\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 184\n",
      "[LightGBM] [Info] Start training from score 12.019795\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3354\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 181\n",
      "[LightGBM] [Info] Start training from score 12.020663\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002756 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3350\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 180\n",
      "[LightGBM] [Info] Start training from score 12.026297\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002699 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3364\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 183\n",
      "[LightGBM] [Info] Start training from score 12.029436\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3359\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 183\n",
      "[LightGBM] [Info] Start training from score 12.022123\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002730 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3360\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 183\n",
      "[LightGBM] [Info] Start training from score 12.023877\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002076 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3364\n",
      "[LightGBM] [Info] Number of data points in the train set: 1314, number of used features: 182\n",
      "[LightGBM] [Info] Start training from score 12.022444\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, shuffle=True, random_state=rstate)\n",
    "oof_preds = {name: np.zeros(len(X), dtype=float) for name in base_models}\n",
    "oof_idx_mask = np.zeros(len(X), dtype=bool)\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(kf.split(X, y), 1):\n",
    "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "    pre = build_preprocessor(X_tr)\n",
    "    fold_preds = {}\n",
    "    for name, mdl in base_models.items():\n",
    "        pipe = Pipeline([(\"pre\", clone(pre)), (\"model\", clone(mdl))])\n",
    "        pipe.fit(X_tr, y_tr)\n",
    "        p = pipe.predict(X_va)\n",
    "        fold_preds[name] = p\n",
    "    p_ens = np.mean(np.column_stack([fold_preds[n] for n in base_models]), axis=1)\n",
    "    rmse = root_mean_squared_error(y_va, p_ens)\n",
    "    r2 = r2_score(y_va, p_ens)\n",
    "    fold_metrics.append({\"fold\": fold, \"rmse\": float(rmse), \"r2\": float(r2)})\n",
    "    for name in base_models:\n",
    "        oof_preds[name][va_idx] = fold_preds[name]\n",
    "    oof_idx_mask[va_idx] = True\n",
    "\n",
    "cv_rmse_mean = float(np.mean([m[\"rmse\"] for m in fold_metrics]))\n",
    "cv_rmse_std  = float(np.std([m[\"rmse\"] for m in fold_metrics]))\n",
    "cv_r2_mean   = float(np.mean([m[\"r2\"] for m in fold_metrics]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d69da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ac20dc",
   "metadata": {},
   "source": [
    "Optimizacion de los pesos del ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ad9e8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003638 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3461\n",
      "[LightGBM] [Info] Number of data points in the train set: 1460, number of used features: 187\n",
      "[LightGBM] [Info] Start training from score 12.024057\n"
     ]
    }
   ],
   "source": [
    "grid_step = 0.05\n",
    "best = {\"weights\": None, \"rmse\": np.inf, \"r2\": -np.inf}\n",
    "M = np.column_stack([oof_preds[\"elasticnet\"], oof_preds[\"lgbm\"]])\n",
    "y_true = y.values\n",
    "\n",
    "for w1 in np.arange(0, 1 + 1e-9, grid_step):\n",
    "            w2 = 1 - w1\n",
    "            y_pred = w1 * oof_preds[\"elasticnet\"] + w2 * oof_preds[\"lgbm\"]\n",
    "            rmse = root_mean_squared_error(y_true, y_pred)\n",
    "            r2 = r2_score(y_true, y_pred)\n",
    "            if rmse < best[\"rmse\"]:\n",
    "                best[\"weights\"] = (w1, w2)\n",
    "                best[\"rmse\"] = rmse\n",
    "                best[\"r2\"] = r2\n",
    "\n",
    "opt_weights = {\"elasticnet\": best[\"weights\"][0], \"lgbm\": best[\"weights\"][1]}\n",
    "\n",
    "pre_final = build_preprocessor(X)\n",
    "final_pipes = {}\n",
    "for name, mdl in base_models.items():\n",
    "    pipe = Pipeline([(\"pre\", clone(pre_final)), (\"model\", clone(mdl))])\n",
    "    pipe.fit(X, y)\n",
    "    final_pipes[name] = pipe\n",
    "\n",
    "for model in final_pipes:\n",
    "    model_path = os.path.join('../model_ensemble/ensemble_model/', f\"model_{model}.pkl\")\n",
    "    joblib.dump(final_pipes[model], model_path)\n",
    "\n",
    "weights_json = \"../model_ensemble/ensemble_weights/weights.json\"\n",
    "json.dump({\n",
    "    \"w_elnet\": float(opt_weights[\"elasticnet\"]),\n",
    "    \"w_lgbm\":  float(opt_weights[\"lgbm\"])\n",
    "}, open(weights_json, \"w\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f60e5b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723ba47",
   "metadata": {},
   "source": [
    "Creamos y guardamos con MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "899c33e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MLflow] Tracking URI: file:/Users/yosesotomayor/Code/retoCasas/.ML\n",
      "[MLflow] Registry URI: file:/Users/yosesotomayor/Code/retoCasas/.ML\n",
      "[MLflow] Experimento:  HousePrices-Competition (id=116606298190487712)\n",
      "[MLflow] Artifact loc:  file:///Users/yosesotomayor/Code/retoCasas/.ML/116606298190487712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/08/27 17:30:38 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/08/27 17:30:38 INFO mlflow.pyfunc: Validating input example against model signature\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 871.27it/s] \n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 1468.08it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 229.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URI del modelo: runs:/1650df3d3ff24d328ac0e365a0283534/ensemble_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 2/2 [00:00<00:00, 76.19it/s]  \n",
      "Downloading artifacts: 100%|██████████| 10/10 [00:00<00:00, 1505.12it/s]\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 3013.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado en: ../model_ensemble/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "exp_id = setup_mlflow(\n",
    "    experiment=\"HousePrices-Competition\", \n",
    "    description=\"ElasticNet+LGBM_Ensemble\", \n",
    "    experiment_tags={\"Kaggle-Model\": \"ElasticNet+LGBM_Ensemble\"}\n",
    ")\n",
    "\n",
    "prediction_example = pd.read_csv(\"../../../../data/housing_data/sample_submission.csv\").drop([\"Id\"], axis=1)\n",
    "ensemble = WeightedEnsemble()\n",
    "signature = infer_signature(X_test, prediction_example)\n",
    "\n",
    "with mlflow.start_run(run_name=\"ElasticNet+LGBM_Ensemble\", experiment_id=exp_id) as run:\n",
    "    \n",
    "    try:\n",
    "        enet_est = getattr(final_pipes[\"elasticnet\"], \"named_steps\", {}).get(\"model\") \\\n",
    "            or final_pipes[\"elasticnet\"].named_steps[list(final_pipes[\"elasticnet\"].named_steps.keys())[-1]]\n",
    "        lgbm_est = getattr(final_pipes[\"lgbm\"], \"named_steps\", {}).get(\"model\") \\\n",
    "            or final_pipes[\"lgbm\"].named_steps[list(final_pipes[\"lgbm\"].named_steps.keys())[-1]]\n",
    "        mlflow.log_param(\"random_state\", rstate)\n",
    "        mlflow.log_param(\"elasticnet_alpha\", getattr(enet_est, \"alpha\", None))\n",
    "        mlflow.log_param(\"elasticnet_l1_ratio\", getattr(enet_est, \"l1_ratio\", None))\n",
    "        mlflow.log_param(\"lgbm_n_estimators\", getattr(lgbm_est, \"n_estimators\", None))\n",
    "        mlflow.log_param(\"lgbm_max_depth\", getattr(lgbm_est, \"max_depth\", None))\n",
    "        mlflow.log_param(\"lgbm_num_leaves\", getattr(lgbm_est, \"num_leaves\", None))\n",
    "        mlflow.log_param(\"lgbm_learning_rate\", getattr(lgbm_est, \"learning_rate\", None))\n",
    "        mlflow.log_param(\"lgbm_subsample\", getattr(lgbm_est, \"subsample\", None))\n",
    "        mlflow.log_param(\"lgbm_colsample_bytree\", getattr(lgbm_est, \"colsample_bytree\", None))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    train_ds = mlflow.data.from_pandas(df_train, source=\"data/housing_data/train.csv\")\n",
    "    mlflow.log_input(train_ds, context=\"training\")\n",
    "    \n",
    "    test_ds = mlflow.data.from_pandas(df_test, source=\"data/housing_data/test.csv\")\n",
    "    mlflow.log_input(test_ds, context=\"inference\")\n",
    "    \n",
    "    mlflow.log_param(\"n_features\", X_test.shape[1])\n",
    "    mlflow.log_param(\"ensemble_weights_elasticnet\", opt_weights[\"elasticnet\"])\n",
    "    mlflow.log_param(\"ensemble_weights_lgbm\", opt_weights[\"lgbm\"])\n",
    "    \n",
    "    mlflow.log_metric(\"cv_rmse_mean\", cv_rmse_mean)\n",
    "    mlflow.log_metric(\"cv_rmse_std\", cv_rmse_std)\n",
    "    mlflow.log_metric(\"cv_r2_mean\", cv_r2_mean)\n",
    "    \n",
    "    mlflow.pyfunc.log_model(\n",
    "        python_model = ensemble,\n",
    "        artifacts={\n",
    "            \"weights_json\": weights_json,\n",
    "            \"model_elasticnet_pkl\": \"../model_ensemble/ensemble_model/model_elasticnet.pkl\",\n",
    "            \"model_lgbm_pkl\": \"../model_ensemble/ensemble_model/model_lgbm.pkl\"\n",
    "        },\n",
    "        signature=signature,\n",
    "        artifact_path=\"ensemble_model\",\n",
    "        input_example=X_test.head(),\n",
    "    )\n",
    "    mlflow.log_artifact(train_data, artifact_path=\"data\")\n",
    "    mlflow.log_artifact(test_data, artifact_path=\"data\")\n",
    "\n",
    "    mlflow.log_artifact(weights_json, artifact_path=\"ensemble_weights\")\n",
    "    mlflow.log_artifact(\"../model_ensemble/ensemble_model/model_elasticnet.pkl\", artifact_path=\"ensemble_model\")\n",
    "    mlflow.log_artifact(\"../model_ensemble/ensemble_model/model_lgbm.pkl\", artifact_path=\"ensemble_model\")\n",
    "\n",
    "    mlflow.set_tags({\n",
    "        \"task\": \"regression\",\n",
    "        \"dataset\": \"Kaggle-HousePrices-Competition\",\n",
    "        \"target\": \"SalePrice\",\n",
    "        \"target_transform\": \"log1p\",\n",
    "        \"inference_transform\": \"expm1\",\n",
    "        \"ensemble\": \"Weighted_ElasticNet+LGBM\"\n",
    "    })\n",
    "    MODEL_URI = f\"runs:/{run.info.run_id}/ensemble_model\"\n",
    "    print(\"URI del modelo:\", f\"runs:/{run.info.run_id}/ensemble_model\")\n",
    "\n",
    "client = MlflowClient()\n",
    "remote_path = ['ensemble_model', 'ensemble_weights']\n",
    "local_dir = \"../model_ensemble/\"\n",
    "for path in remote_path:\n",
    "    client.download_artifacts(run_id=run.info.run_id, path=path, dst_path=local_dir)\n",
    "\n",
    "print(\"Guardado en:\", local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26985dcc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a5fef",
   "metadata": {},
   "source": [
    "Mostramos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89b55b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESUMEN CV ===\n",
      "RMSE (mean ± std): 0.11708 ± 0.02141\n",
      "R2   (mean):       0.90927\n",
      "Pesos óptimos (OOF): {'elasticnet': np.float64(0.5), 'lgbm': np.float64(0.5)}\n",
      "Modelos y artefactos registrados en MLflow (ver en la UI del tracking server).\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== RESUMEN CV ===\")\n",
    "print(f\"RMSE (mean ± std): {cv_rmse_mean:.5f} ± {cv_rmse_std:.5f}\")\n",
    "print(f\"R2   (mean):       {cv_r2_mean:.5f}\")\n",
    "print(\"Pesos óptimos (OOF):\", opt_weights)\n",
    "print(\"Modelos y artefactos registrados en MLflow (ver en la UI del tracking server).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8717e8ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5e5962",
   "metadata": {},
   "source": [
    "### Deployment del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b36a85",
   "metadata": {},
   "source": [
    "Registrar versiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7096c2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'HousePrices_Ensemble' already exists. Creating a new version of this model...\n",
      "2025/08/27 17:30:59 WARNING mlflow.tracking._model_registry.fluent: Run with id 1650df3d3ff24d328ac0e365a0283534 has no artifacts at artifact path 'ensemble_model', registering model based on models:/m-23f6cba0541a4eaa897eb52795475501 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version registrada: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'HousePrices_Ensemble'.\n"
     ]
    }
   ],
   "source": [
    "mv = mlflow.register_model(\n",
    "    model_uri = MODEL_URI,\n",
    "    name = MODEL_NAME,\n",
    "    tags = {\n",
    "        \"task\": \"regression\",\n",
    "        \"dataset\": \"Kaggle-HousePrices-Competition\",\n",
    "        \"target\": \"SalePrice\",\n",
    "        \"target_transform\": \"log1p\",\n",
    "        \"inference_transform\": \"expm1\",\n",
    "        \"ensemble\": \"Weighted_ElasticNet+LGBM\"\n",
    "    },\n",
    ")\n",
    "\n",
    "client.set_registered_model_tag(mv.name, \"created_by\", \"Yose Sotomayor\")\n",
    "client.set_registered_model_alias(mv.name, \"challenger\", version=mv.version)\n",
    "print(\"Version registrada:\", mv.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0852be",
   "metadata": {},
   "source": [
    "Transicionar versiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd2eb6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_registered_model_alias(MODEL_NAME, \"challenger\")\n",
    "client.set_registered_model_alias(MODEL_NAME, \"champion\", mv.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a696c5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90d75d",
   "metadata": {},
   "source": [
    "Guardar los datos para subir a Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a126865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(f\"models:/{MODEL_NAME}@champion\")\n",
    "\n",
    "preds_orig  = model.predict(X_test)\n",
    "\n",
    "sub_dir = \"../../../data/housing_submissions/elnet_lgbm\"\n",
    "os.makedirs(sub_dir, exist_ok=True)\n",
    "submission_path = os.path.join(sub_dir, \"submission_elnet_lgbm.csv\")\n",
    "\n",
    "\n",
    "df_sub = pd.DataFrame({\"Id\": df_test[\"Id\"], \"SalePrice\": preds_orig})\n",
    "df_sub.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7df61a",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reto_housing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
